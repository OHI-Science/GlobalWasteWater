{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "####\n",
    "####   Pour Point Rst\n",
    "####   By Cascade Tuholske June 2020\n",
    "####\n",
    "##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dependencies \n",
    "##############################################################################################################\n",
    "from rasterstats import zonal_stats, gen_zonal_stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import time\n",
    "import multiprocessing as mp \n",
    "from multiprocessing import Pool\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effluent rsts\n",
    "DATA_IN = '/home/cascade/projects/wastewater/data/'\n",
    "effluent_rsts = [DATA_IN+'interim/effluent_N_mask.tif', DATA_IN+'interim/effluent_N_treated_mask.tif', \n",
    "                 DATA_IN+'interim/effluent_N_septic_mask.tif', DATA_IN+'interim/effluent_N_open_mask.tif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effluent rsts\n",
    "DATA_IN = '/home/cascade/projects/wastewater/data/'\n",
    "effluent_rsts = [DATA_IN+'interim/effluent_N.tif', DATA_IN+'interim/effluent_N_treated.tif', \n",
    "                 DATA_IN+'interim/effluent_N_septic.tif', DATA_IN+'interim/effluent_N_open.tif']\n",
    "\n",
    "# Get one raster\n",
    "rst = effluent_rsts[0]\n",
    "\n",
    "# Files and Paths\n",
    "data = rst.split('interim/')[1].split('.')[0].split('mask64')[0]\n",
    "DATA_IN = rst.split('interim/')[0]\n",
    "path_out = 'processed/N_effluent_output/'\n",
    "fn_out = DATA_IN+path_out+data\n",
    "print('fn_out', fn_out)\n",
    "\n",
    "# Open countries    \n",
    "countries_fn = DATA_IN+'interim/world_vector.shp' \n",
    "countries = gpd.read_file(countries_fn)\n",
    "\n",
    "# Open watershed basins polygons and stack em\n",
    "basins_dir = glob(DATA_IN+'interim/basins_crs/*59004.shp')\n",
    "\n",
    "# empty df to stack all the watershed polys\n",
    "columns= (['ID','GRIDCODE','inspect','area','PNTPOLYCNT','basin_id','MWa_in_km2','geometry'])\n",
    "watersheds = pd.DataFrame(columns = columns)\n",
    "\n",
    "# Open watershed polys\n",
    "for shp_fn in basins_dir:\n",
    "    basins = pd.DataFrame(gpd.read_file(shp_fn))\n",
    "    watersheds = watersheds.append(basins, sort = False)\n",
    "\n",
    "watersheds = gpd.GeoDataFrame(watersheds) # to geo data frame\n",
    "watersheds.crs = countries.crs # Set crs\n",
    "\n",
    "# Drop inland watersheds w/ pourpoints file\n",
    "inland_pp_fn = DATA_IN+'interim/pourpoints_inland.shp'\n",
    "inland_pp = gpd.read_file(inland_pp_fn)\n",
    "inland_pp = inland_pp[inland_pp['land-ocean'] == 0] # drop inland watersheds \n",
    "inland_pp = inland_pp['basin_id']\n",
    "watersheds = watersheds.merge(inland_pp, on = 'basin_id', how = 'inner') # merge in\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN = '/home/cascade/projects/wastewater/data/'\n",
    "MASK_FN = DATA_IN+'interim/inlandwatersheds_mask.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zonal(rst_fn, polys, fn_out, geog): \n",
    "    \"\"\"Function will run zonal stats on countries or watersheds. Only use watersheds \n",
    "    that have had the had the inland watersheds dropped. \n",
    "    \n",
    "    Args:\n",
    "        rst_fn = file name/path of raster to run zonal stats on\n",
    "        geog = countries or watersheds as str for fn_out\n",
    "        polys = either list of shape files (watersheds) or single shape file (countries)\n",
    "        fn_out = file and path for shp and csv file\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Run Zonal Stats\n",
    "    zs_feats = zonal_stats(polys, rst_fn, stats=\"sum count\", geojson_out=True)\n",
    "        \n",
    "    # Turn into geo data frame and rename column\n",
    "    zgdf = gpd.GeoDataFrame.from_features(zs_feats, crs=watersheds.crs)\n",
    "    zgdf = zgdf.rename(columns={'sum': 'effluent'})\n",
    "    zgdf.effluent = zgdf.effluent.fillna(0)\n",
    "    \n",
    "    # Save out shape and CSV\n",
    "    zgdf.to_file(fn_out+geog+'.shp')\n",
    "    zgdf.to_csv(fn_out+geog'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Open countries\n",
    "print('Opening countries')\n",
    "countries_fn = DATA_IN+'interim/world_vector.shp' \n",
    "countries = gpd.read_file(countries_fn)\n",
    "print('countries are: ', len(countries))\n",
    "\n",
    "# Open watershed basins polygons and stack em\n",
    "print('Opening watersheds')\n",
    "basins_dir = glob(DATA_IN+'interim/basins_crs/*59004.shp')\n",
    "\n",
    "# empty df to stack all the watershed polys\n",
    "columns= (['ID','GRIDCODE','inspect','area','PNTPOLYCNT','basin_id','MWa_in_km2','geometry'])\n",
    "watersheds = pd.DataFrame(columns = columns)\n",
    "\n",
    "# Open watershed polys\n",
    "for shp_fn in basins_dir:\n",
    "    basins = pd.DataFrame(gpd.read_file(shp_fn))\n",
    "    watersheds = watersheds.append(basins, sort = False)\n",
    "\n",
    "watersheds = gpd.GeoDataFrame(watersheds) # to geo data frame\n",
    "watersheds.crs = countries.crs # Set crs\n",
    "print('watersheds are:', len(watersheds), type(watersheds), watersheds.crs)\n",
    "\n",
    "# Drop inland watersheds w/ pourpoints file\n",
    "inland_pp_fn = DATA_IN+'interim/pourpoints_inland.shp'\n",
    "inland_pp = gpd.read_file(inland_pp_fn)\n",
    "inland_pp = inland_pp[inland_pp['land-ocean'] == 0] # drop inland watersheds \n",
    "inland_pp = inland_pp['basin_id']\n",
    "watersheds = watersheds.merge(inland_pp, on = 'basin_id', how = 'inner') # merge in\n",
    "print('watersheds are:', len(watersheds), type(watersheds), watersheds.crs)\n",
    "\n",
    "# Mask64 rasters\n",
    "effluent_rsts = glob(DATA_IN+'interim/*mask64.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask64 rasters\n",
    "effluent_rsts = glob(DATA_IN+'interim/*mask64.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_par(rst):\n",
    "    \"\"\"UPDATE\"\"\"\n",
    "    \n",
    "    # print current process\n",
    "    print(mp.current_process())\n",
    "    \n",
    "    # Files and Paths\n",
    "    data = rst.split('interim/')[1].split('.')[0].split('mask64')[0]\n",
    "    DATA_IN = rst.split('interim/')[0]\n",
    "    path_out = 'processed/N_effluent_output/'\n",
    "    geog = 'watersheds'\n",
    "    fn_out = DATA_IN+path_out+data+geog\n",
    "    print('fn_out', fn_out)\n",
    "    \n",
    "    # Open countries    \n",
    "    countries_fn = DATA_IN+'interim/world_vector.shp' \n",
    "    countries = gpd.read_file(countries_fn)\n",
    "\n",
    "    # Open watershed basins polygons and stack em\n",
    "    basins_dir = glob(DATA_IN+'interim/basins_crs/*59004.shp')\n",
    "\n",
    "    # empty df to stack all the watershed polys\n",
    "    columns= (['ID','GRIDCODE','inspect','area','PNTPOLYCNT','basin_id','MWa_in_km2','geometry'])\n",
    "    watersheds = pd.DataFrame(columns = columns)\n",
    "\n",
    "    # Open watershed polys\n",
    "    for shp_fn in basins_dir:\n",
    "        basins = pd.DataFrame(gpd.read_file(shp_fn))\n",
    "        watersheds = watersheds.append(basins, sort = False)\n",
    "\n",
    "    watersheds = gpd.GeoDataFrame(watersheds) # to geo data frame\n",
    "    watersheds.crs = countries.crs # Set crs\n",
    "\n",
    "    # Drop inland watersheds w/ pourpoints file\n",
    "    inland_pp_fn = DATA_IN+'interim/pourpoints_inland.shp'\n",
    "    inland_pp = gpd.read_file(inland_pp_fn)\n",
    "    inland_pp = inland_pp[inland_pp['land-ocean'] == 0] # drop inland watersheds \n",
    "    inland_pp = inland_pp['basin_id']\n",
    "    watersheds = watersheds.merge(inland_pp, on = 'basin_id', how = 'inner') # merge in\n",
    "    \n",
    "    # country zonal stats\n",
    "    # print('Starting country', rst)\n",
    "    #run_zonal(rst, polys = countries, fn_out = fn_out)\n",
    "    \n",
    "    # watershed zonal stats\n",
    "    print('Starting watershed', rst)\n",
    "    run_zonal(rst, polys = watersheds, fn_out = fn_out)\n",
    "    \n",
    "#     print(data, 'DONE \\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start pools\n",
    "def parallel_loop(function, job_list, cpu_num):\n",
    "    \"\"\"Run the temp-ghs routine in parallel\n",
    "    Args: \n",
    "        function = function to apply in parallel\n",
    "        job_list = list of dir or fn to loop through \n",
    "        cpu_num = numper of cpus to fire  \n",
    "    \"\"\" \n",
    "    start = time.time()\n",
    "    pool = Pool(processes = cpu_num)\n",
    "    pool.map(function, job_list)\n",
    "    # pool.map_async(function, dir_list)\n",
    "    pool.close()\n",
    "\n",
    "    end = time.time()\n",
    "    print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run it\n",
    "parallel_loop(zonal_par, effluent_rsts, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to stack and merge it all together. \n",
    "# FIX geog in zonal stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
