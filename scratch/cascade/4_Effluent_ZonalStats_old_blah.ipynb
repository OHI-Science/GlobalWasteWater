{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effluent Totals\n",
    "\n",
    "Notebook that can be turned into a python script to caluclate zonal stats for effluent totals for each watershed and then connect them to pour points (taken from Jared's python and then R script).\n",
    "\n",
    "The watersheds are in different CRS and thus cannot be stacked. They will be converted to espg 54009, which will make some coastal issues, but on the whole this is the best we can do.\n",
    "\n",
    "By Cascade Tuholske 2019-11-11\n",
    "\n",
    "**This is Jared's code adpoted**\n",
    "\n",
    "This works but be sure to change the file paths and names for FIO and N accordingly - CPT 2020.02.02\n",
    "\n",
    "**UPDATED 2020-03-23** <br>\n",
    "This needs to be run for total N and each treatment type (open, septic, and treated) for\n",
    "watershed-level attribution <br>\n",
    "Country-level data added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dependencies\n",
    "from rasterstats import zonal_stats, gen_zonal_stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### File Paths and Files\n",
    "##############################################################################################################\n",
    "\n",
    "DATA_IN = '/home/cascade/projects/wastewater/data/'\n",
    "MASK_FN = DATA_IN+'interim/inlandwatersheds_mask.tif'\n",
    "\n",
    "#### Make Masked Effluent Rasters \n",
    "##############################################################################################################\n",
    "\n",
    "def mask_effluent(effluent_fn, mask_fn, out_fn):\n",
    "    \n",
    "    \"\"\"Function opens effluent raster and masks inland watershed pixels\\\n",
    "    Args:\n",
    "        effluent_fn = effluent raster fn\n",
    "        mask_fn = mask raster fn\n",
    "        out_fn = file name out\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open effluent raster\n",
    "    rst = rasterio.open(effluent_fn)\n",
    "    meta = rst.meta\n",
    "    \n",
    "    # Update Data Type\n",
    "    meta.update({'dtype' : 'float64'}) \n",
    "    band = rst.read(1)\n",
    "    band = band.astype('float64')\n",
    "    \n",
    "    # mask inland watersheds\n",
    "    mask = rasterio.open(mask_fn).read(1)\n",
    "    mask[mask == 1] = 2 # revalue mask so inland watersheds are = 0 \n",
    "    mask[mask == 0] = 1\n",
    "    mask[mask == 2] = 0\n",
    "    \n",
    "    band_out = band * mask \n",
    "\n",
    "    # Save new data type and mask out\n",
    "    with rasterio.open(out_fn, 'w', **meta) as dst:\n",
    "        dst.write(band_out, 1)\n",
    "    \n",
    "    print('Done', out_fn)\n",
    "\n",
    "# effluent rsts\n",
    "effluent_rsts = [DATA_IN+'interim/effluent_N.tif', DATA_IN+'interim/effluent_N_treated.tif', \n",
    "                 DATA_IN+'interim/effluent_N_septic.tif', DATA_IN+'interim/effluent_N_open.tif']\n",
    "\n",
    "# Make masked rasters\n",
    "for rst in effluent_rsts:\n",
    "    \n",
    "    # Get data type\n",
    "    data = rst.split('interim/')[1].split('.')[0]\n",
    "    print(data)\n",
    "\n",
    "    # Raster Mask 64 \n",
    "    print('Starting mask64', rst)\n",
    "    rst_out = DATA_IN+'interim/'+data+'_mask64.tif'\n",
    "    mask_effluent(rst, MASK_FN, rst_out)\n",
    "\n",
    "#### Zonal Stats\n",
    "##############################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watersheds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### File Paths on CPT Home\n",
    "# Basins have already been reprojected to EPSG 59004\n",
    "data = 'N_treated' # name of data: N, open, septic, treated \n",
    "data_type = 'N' # FIO or N in directory name\n",
    "fn_in = \"effluent_\"+data\n",
    "data_dir = \"/home/cascade/projects/wastewater/data/interim/\"\n",
    "data_out =  \"/home/cascade/projects/wastewater/data/interim/\"+data_type+\"_effluent_output/\"\n",
    "basins_dir = data_dir+\"basins_crs/\"\n",
    "shps = [os.path.join(basins_dir, fn) for fn in os.listdir(basins_dir) if fn.endswith(\"59004.shp\")]\n",
    "effluent_fn = os.path.join(data_dir, fn_in+'.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First need to recast rasters as float64\n",
    "\n",
    "rst = rasterio.open(effluent_fn)\n",
    "meta = rst.meta\n",
    "meta.update({'dtype' : 'float64'}) \n",
    "band = rst.read(1)\n",
    "band = band.astype('float64')\n",
    "\n",
    "with rasterio.open(data_dir+fn_in+'64.tif', 'w', **meta) as dst:\n",
    "    dst.write(band, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Zonal Stats\n",
    "effluent_fn = os.path.join(data_dir, fn_in+'64.tif') # update for float64\n",
    "\n",
    "feature_list = []\n",
    "\n",
    "for shp_fn in shps:\n",
    "    watersheds = gpd.read_file(shp_fn)\n",
    "    zs_feats = zonal_stats(watersheds, effluent_fn, stats=\"sum count\", geojson_out=True)\n",
    "    feature_list.extend(zs_feats)\n",
    "    print(shp_fn, ' is done')\n",
    "    \n",
    "zgdf = gpd.GeoDataFrame.from_features(feature_list, crs=watersheds.crs)\n",
    "zgdf = zgdf.rename(columns={'sum': 'effluent'})\n",
    "zgdf.effluent = zgdf.effluent.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zgdf.to_file(data_out+'effluent_'+data+'_watersheds.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zgdf.to_csv(data_out+'effluent_'+data+'_watersheds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = gpd.read_file(data_out+'effluent_'+data+'_watersheds.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check.sort_values(['effluent'], ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country-level\n",
    "Completed by Cascade Tuholske 2020.03.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basins have already been reprojected to EPSG 59004\n",
    "data = 'N_septic' # name of data: N, open, septic, treated \n",
    "data_type = 'N' # FIO or N in directory name\n",
    "fn_in = \"effluent_\"+data\n",
    "data_dir = \"/home/cascade/projects/wastewater/data/interim/\"\n",
    "data_out =  \"/home/cascade/projects/wastewater/data/interim/\"+data_type+\"_effluent_output/\"\n",
    "countries_shps_fn = 'world_vector.shp'\n",
    "effluent_fn = os.path.join(data_dir, fn_in+'.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First need to recast rasters as float64\n",
    "rst = rasterio.open(effluent_fn)\n",
    "meta = rst.meta\n",
    "meta.update({'dtype' : 'float64'}) \n",
    "band = rst.read(1)\n",
    "band = band.astype('float64')\n",
    "\n",
    "with rasterio.open(data_dir+fn_in+'64.tif', 'w', **meta) as dst:\n",
    "    dst.write(band, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "\n",
    "effluent_fn = os.path.join(data_dir, fn_in+'64.tif') # update for float64\n",
    "countries = gpd.read_file(data_dir+countries_shps_fn) # load countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Zonal Stats\n",
    "zs_feats = zonal_stats(countries, effluent_fn, stats=\"sum count\", geojson_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write to a dataframe  \n",
    "zgdf = gpd.GeoDataFrame.from_features(zs_feats, crs=countries.crs)\n",
    "zgdf = zgdf.rename(columns={'sum': 'effluent'})\n",
    "zgdf.effluent = zgdf.effluent.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zgdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out\n",
    "\n",
    "zgdf.to_file(data_out+'effluent_'+data+'_countries.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = gpd.read_file(data_out+'effluent_'+data+'_countries.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check.sort_values(['effluent'], ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterstats import zonal_stats, gen_zonal_stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### File Paths on CPT Home\n",
    "# Basins have already been reprojected to 59004\n",
    "data_dir = \"/Users/cascade/Github/wastewater_ohi/data/interim/\"\n",
    "data_out =  \"/Users/cascade/Github/wastewater_ohi/data/processed/FIO_effluent_output/\"\n",
    "basins_dir = os.path.join(data_dir, \"basins_crs/\")\n",
    "shps = [os.path.join(basins_dir, fn) for fn in os.listdir(basins_dir) if fn.endswith(\"59004.shp\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is from my ERL paper, it should work for the GHS\n",
    "# https://github.com/cascadet/AfricaUrbanPop/blob/master/notebooks/jupyter/ERL19/Step4_Zonal_Stats.ipynb \n",
    "# Update - this dict is the correct for espg 54009, which is not in fiona, but works fine.\n",
    "# see this for more details: https://epsg.io/54009\n",
    "\n",
    "new_crs = {'proj': 'moll', 'lon_0': 0, 'x_0': 0, 'y_0': 0, 'ellps': 'WGS84', 'units': 'm', 'no_defs': True}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Geom to write out files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get geometry for watersheds to write out \n",
    "\n",
    "geom_out = []\n",
    "basin_id_list = []\n",
    "area_list = []\n",
    "for shp_fn in shps: \n",
    "    shp_fn = gpd.read_file(shp_fn)# .to_crs(new_crs) # switches them all to espg 54009 ... cpt 2020.01.24 not needed\n",
    "    basin_id = shp_fn['basin_id']\n",
    "    geom = shp_fn['geometry']\n",
    "    area = shp_fn['area']\n",
    "    basin_id_list.extend(basin_id)\n",
    "    geom_out.extend(geom)\n",
    "    area_list.extend(area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Dataframe to write out\n",
    "\n",
    "out_shape = gpd.GeoDataFrame()\n",
    "out_shape['geometry'] = geom_out\n",
    "out_shape['basin_id'] = basin_id_list\n",
    "out_shape['area'] = area_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run zonal stats\n",
    "\n",
    "Jared's code does not seem to be working. Producing -inf and NAs, going to try my code from the Africa project\n",
    "\n",
    "**Be sure to switch l, m, h files since I did not write it as a loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2020.01.24 CPT --- make \n",
    "\n",
    "col = 'FIO'\n",
    "effluent_fn = os.path.join(data_dir, \"effluent_FIO.tif\")\n",
    "#output_fn = os.path.join(data_out, \"FIO_sep0_effluent_watersheds_all.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check CRS first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Check crs of .tif and shape files\n",
    "\n",
    "shp_test = gpd.read_file(shps[0])\n",
    "print('Shape crs \\n ', shp_test.crs)\n",
    "rst_test = rasterio.open(effluent_fn)\n",
    "print('Raster crs \\n', rst_test.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shp_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zone_stat(raster, band, polygon, stats, touched):\n",
    "    \"\"\"\n",
    "    This function will calculate the zonal stats for each polygon within a raster\n",
    "    requires gpd_df, raster, object and nodata value\n",
    "    \n",
    "    Args: raster = input raster\n",
    "          band = band of raster\n",
    "          polygon = polygons to calc zonal stats \n",
    "          stats = stat to calculate as string\n",
    "          touched = True or False, to include pixels intersected w/ polygons\n",
    "    \"\"\"\n",
    "    \n",
    "    band = raster.read(band)\n",
    "    band[band < 0] = 0 # Fix missing data\n",
    "    zone_stat = zonal_stats(polygon, band, affine=raster.meta['transform'], \n",
    "                            nodata = -3.4e+38, stats = stats, all_touched = touched)\n",
    "    return zone_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calc Zonal Stats\n",
    "### Running sontal stats with all touched = True https://pythonhosted.org/rasterstats/manual.html#statistics\n",
    "\n",
    "rst = rasterio.open(effluent_fn) # Open raster\n",
    "feature_list = []\n",
    "\n",
    "for shp_fn in shps:\n",
    "    watersheds = gpd.read_file(shp_fn).to_crs(new_crs) \n",
    "    zs_feats = zone_stat(rst, 1, watersheds, 'sum', True)\n",
    "    feature_list.extend(zs_feats)\n",
    "    print('One shape is done')\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Nans and set to log scale if desired \n",
    "\n",
    "out_shape[col] = pd.DataFrame.from_dict(feature_list)\n",
    "out_shape[col] = out_shape[[col]].replace(0, np.nan) # Set zeros to NAN, can run as log if needed\n",
    "out_shape[col] = out_shape[col].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out a sub-set\n",
    "\n",
    "# Dropping all the tiny tiny fractional watersheds \n",
    "\n",
    "out_shape_sub = out_shape[out_shape[col] >0]\n",
    "print(len(out_shape_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "out_shape_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out \n",
    "out_shape.to_file(data_out+col+'_effluent_watersheds.shp')\n",
    "out_shape_sub.to_file(data_out+col+'_effluent_watersheds_sub.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pour_points = gpd.read_file(data_dir+'global_plume_2007_2010.shp') # Open Pour Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch crs to match ocean mask\n",
    "print(pour_points.crs)\n",
    "pour_points = pour_points.to_crs({'init': 'epsg:4326'})\n",
    "print(pour_points.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pour_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join Watershed Effluent Values to Pour Points\n",
    "print(len(pour_points))\n",
    "pp_merge_all = out_shape.drop(columns = 'geometry')\n",
    "pp_merge_all = pd.merge(pp_merge_all, pour_points, on = 'basin_id', how = 'inner') # <<--- one gets dropped\n",
    "print(len(pp_merge_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_merge_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join Watershed Effluent Values to Pour Points\n",
    "print(len(pour_points))\n",
    "pp_merge_sub = out_shape_sub.drop(columns = 'geometry')\n",
    "pp_merge_sub = pd.merge(pp_merge_sub, pour_points, on = 'basin_id', how = 'inner') # <<--- one gets dropped\n",
    "print(len(pp_merge_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp_merge_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write out pour points\n",
    "\n",
    "pp_merge_all.to_file(data_out+col+'_pour_point_totals_all.shp')\n",
    "pp_merge_sub.to_file(data_out+col+'_pour_point_totals_sub.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour point data for plumes\n",
    "\n",
    "From the pilot, we need pour points with the following columns plus geometry in the following order:\n",
    "gemometry, basin_id, FIO (e.g. effluent) and area.\n",
    "\n",
    "I am going to use the sub-pours file or effluent with >0 to reduce plume processing time\n",
    "CPT 2020.01.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pours = gpd.GeoDataFrame()\n",
    "final_pours['geometry'] = pp_merge_sub['geometry']\n",
    "final_pours['basin_id'] = pp_merge_sub['basin_id']\n",
    "final_pours['effluent'] = pp_merge_sub['FIO'] ## Has to be named effluent based on plume code\n",
    "final_pours['area'] = pp_merge_sub['area']\n",
    "final_pours['count'] = np.nan\n",
    "final_pours.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pours.to_file(data_out+col+'_pourpoints_final.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIO_500 = final_pours[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIO_500.to_file(data_out+col+'_pourpoints_500.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FIO_500.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of FIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test5000 = gpd.read_file(data_out+'FIO_pourpoints_final.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test5000 = test5000[:5000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test5000['area'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test5000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test5000.crs = {'init' :'epsg:4326'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test5000.to_file('~/Desktop/test5000.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out top 100 Watersheds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_merge_sub_sort = pp_merge_sub.sort_values(by = 'Nitrogen', ascending = False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_merge_sub_sort.to_file(data_out+'Nitrogen_pour_point_totals_sub100.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape_sub_sort = out_shape_sub.sort_values(by = 'Nitrogen', ascending = False)[0:100]\n",
    "out_shape_sub_sort.to_file(data_out+'Nitrogen_effluent_watersheds_sub100.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_merge_sub_sort = pp_merge_sub.sort_values(by = 'Nitrogen_area', ascending = False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_merge_sub_sort.to_file(data_out+'Nitrogen_pour_point_area_sub100.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape_sub_sort = out_shape_sub.sort_values(by = 'Nitrogen_area', ascending = False)[0:100]\n",
    "out_shape_sub_sort.to_file(data_out+'Nitrogen_effluent_watersheds_area_sub100.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area and pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effluent by pct\n",
    "\n",
    "out_shape['Nitrogen_pct'] = out_shape['Nitrogen'] / out_shape['Nitrogen'].sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effluent by pct\n",
    "\n",
    "\n",
    "out_shape['Nitrogen_area'] = out_shape['Nitrogen'] / out_shape['area']\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
